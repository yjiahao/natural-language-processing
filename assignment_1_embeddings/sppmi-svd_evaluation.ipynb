{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f597a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16a4013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(vocab: dict, window_size: int, tokenized_sentences: list):\n",
    "    vocab_size = len(vocab)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        for idx, word in enumerate(sentence):\n",
    "            word_idx = vocab[word]\n",
    "            \n",
    "            # Define the context window\n",
    "            start = max(0, idx - window_size)\n",
    "            end = min(sentence_length, idx + window_size + 1)\n",
    "            \n",
    "            # Update co-occurrence counts for words in the window\n",
    "            for context_idx in range(start, end):\n",
    "                if idx != context_idx:  # Skip the word itself\n",
    "                    context_word_idx = vocab[sentence[context_idx]]\n",
    "                    co_matrix[word_idx, context_word_idx] += 1\n",
    "\n",
    "    return co_matrix\n",
    "\n",
    "def create_sppmi_matrix(co_matrix, co_occurrence_sum, word_occurrences, k):\n",
    "    sppmi_matrix = np.zeros_like(co_matrix)\n",
    "    \n",
    "    # Find indices where co_matrix > 0\n",
    "    rows, cols = np.nonzero(co_matrix)\n",
    "\n",
    "    for i, j in zip(rows, cols):\n",
    "        pmi = np.log((co_matrix[i, j] * co_occurrence_sum) / (word_occurrences[i] * word_occurrences[j]))\n",
    "        sppmi = pmi - np.log(k)\n",
    "        sppmi_matrix[i, j] = max(sppmi, 0)  # SPPMI\n",
    "\n",
    "    return sppmi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0bab50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SVD model with dimensions = 128, window size = 5, k = 5\n"
     ]
    }
   ],
   "source": [
    "# load data and get sppmi embeddings again using best SVD model found previously\n",
    "df = pd.read_csv('../data/evaluation/wordsim353crowd.csv')\n",
    "twitter = pd.read_csv('../data/twitter/processed.csv')\n",
    "\n",
    "twitter['processed_text'] = twitter['processed_text'].apply(lambda x: ast.literal_eval(x))\n",
    "tokenized_sentences = list(twitter['processed_text'])\n",
    "\n",
    "vocab = {\n",
    "    word: idx for idx, word in enumerate(set(word for sentence in tokenized_sentences for word in sentence))\n",
    "}\n",
    "\n",
    "co_matrix = create_co_matrix(vocab, 5, tokenized_sentences)\n",
    "co_occurrence_sum = np.sum(co_matrix)\n",
    "word_occurrences = np.sum(co_matrix, axis=1)\n",
    "\n",
    "# create sppmi matrix\n",
    "sppmi_matrix = create_sppmi_matrix(co_matrix, co_occurrence_sum, word_occurrences, 5)\n",
    "\n",
    "# fit svd\n",
    "print(f'Fitting SVD model with dimensions = {128}, window size = {5}, k = {5}')\n",
    "\n",
    "svd = TruncatedSVD(n_components=128)\n",
    "U_k = svd.fit_transform(sppmi_matrix)\n",
    "Sigma_k = np.diag(svd.singular_values_)\n",
    "Sigma_k_sqrt = np.sqrt(Sigma_k)\n",
    "sppmi_embedding = U_k @ Sigma_k_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "016824fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = vocab\n",
    "index_to_word = {idx: word for word, idx in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c22fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    w1, w2 = row['Word 1'], row['Word 2']\n",
    "    if w1 in word_to_index and w2 in word_to_index:\n",
    "        vec1 = sppmi_embedding[word_to_index[w1]].reshape(1, -1)\n",
    "        vec2 = sppmi_embedding[word_to_index[w2]].reshape(1, -1)\n",
    "        sim = cosine_similarity(vec1, vec2)[0][0]\n",
    "        preds.append(sim)\n",
    "    else:\n",
    "        preds.append(np.nan) # result not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e36418c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " 0.18578702,\n",
       " nan,\n",
       " 0.14085941,\n",
       " 0.15019304,\n",
       " 0.14716095,\n",
       " 0.17585915,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.1374014,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.4480145,\n",
       " 0.27687407,\n",
       " 0.19963533,\n",
       " 0.22161505,\n",
       " 0.34761745,\n",
       " nan,\n",
       " nan,\n",
       " 0.014682222,\n",
       " nan,\n",
       " nan,\n",
       " 0.2210814,\n",
       " 0.28948128,\n",
       " 0.33966818,\n",
       " 0.09318803,\n",
       " 0.07934096,\n",
       " 0.27155405,\n",
       " nan,\n",
       " nan,\n",
       " 0.098600656,\n",
       " nan,\n",
       " 0.1083464,\n",
       " 0.55952,\n",
       " nan,\n",
       " 0.08233634,\n",
       " 0.052690953,\n",
       " 0.12460677,\n",
       " 0.11859941,\n",
       " 0.20324986,\n",
       " 0.10720265,\n",
       " nan,\n",
       " 0.26724157,\n",
       " 0.22840117,\n",
       " 0.3352613,\n",
       " 0.20089898,\n",
       " 0.086082816,\n",
       " 0.35240343,\n",
       " 0.067614794,\n",
       " nan,\n",
       " 0.047848225,\n",
       " 0.1989129,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.48950684,\n",
       " 0.31847793,\n",
       " nan,\n",
       " 0.2604121,\n",
       " nan,\n",
       " 0.5176225,\n",
       " 0.11978251,\n",
       " nan,\n",
       " 0.30502585,\n",
       " nan,\n",
       " 0.08127186,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.17026797,\n",
       " 0.40860453,\n",
       " nan,\n",
       " 0.26700854,\n",
       " nan,\n",
       " 0.22248946,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.097981855,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.08082784,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.07917957,\n",
       " nan,\n",
       " 0.3726133,\n",
       " 0.21490696,\n",
       " nan,\n",
       " nan,\n",
       " 0.3611132,\n",
       " 0.0849912,\n",
       " 0.38504252,\n",
       " 0.10786206,\n",
       " 0.29705402,\n",
       " 0.62932265,\n",
       " 0.12199232,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.21760365,\n",
       " nan,\n",
       " 0.281465,\n",
       " 0.4926322,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.01824389,\n",
       " 0.12456763,\n",
       " -0.008830804,\n",
       " 0.3943916,\n",
       " 0.048489198,\n",
       " 0.05526396,\n",
       " 0.10312862,\n",
       " 0.08834869,\n",
       " 0.029315867,\n",
       " 0.033755347,\n",
       " 0.28268915,\n",
       " nan,\n",
       " 0.112835556,\n",
       " 0.114090025,\n",
       " 0.38093743,\n",
       " 0.2863757,\n",
       " 0.19374312,\n",
       " 0.25584495,\n",
       " nan,\n",
       " nan,\n",
       " 0.24480137,\n",
       " 0.07096696,\n",
       " nan,\n",
       " nan,\n",
       " 0.25424492,\n",
       " nan,\n",
       " 0.09572433,\n",
       " nan,\n",
       " 0.02255437,\n",
       " 0.10012945,\n",
       " nan,\n",
       " nan,\n",
       " 0.10247959,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.0480364,\n",
       " nan,\n",
       " nan,\n",
       " 0.3363496,\n",
       " nan,\n",
       " 0.058707267,\n",
       " 0.14326103,\n",
       " 0.09798684,\n",
       " 0.119904235,\n",
       " nan,\n",
       " 0.18724047,\n",
       " 0.27263194,\n",
       " 0.25276968,\n",
       " 0.26265627,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.09622163,\n",
       " nan,\n",
       " -0.069072954,\n",
       " nan,\n",
       " nan,\n",
       " 0.19182247,\n",
       " nan,\n",
       " 0.24971977,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.19058494,\n",
       " nan,\n",
       " nan,\n",
       " 0.22161505,\n",
       " 0.20112205,\n",
       " 0.20112205,\n",
       " nan,\n",
       " nan,\n",
       " 0.3855959,\n",
       " nan,\n",
       " 0.15498395,\n",
       " nan,\n",
       " -0.061776847,\n",
       " 0.025246706,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.071997866,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.23706454,\n",
       " 0.5670599,\n",
       " 0.19541189,\n",
       " nan,\n",
       " nan,\n",
       " 0.1781907,\n",
       " -0.07115665,\n",
       " 0.028106734,\n",
       " 0.014556184,\n",
       " 0.22927567,\n",
       " nan,\n",
       " 0.066815846,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.0026551709,\n",
       " 0.04758723,\n",
       " 0.30867967,\n",
       " 0.19233268,\n",
       " 0.23968838,\n",
       " -0.032921232,\n",
       " nan,\n",
       " nan,\n",
       " 0.3246144,\n",
       " nan,\n",
       " 0.30989644,\n",
       " nan,\n",
       " 0.079462305,\n",
       " 0.12970626,\n",
       " -0.026399367,\n",
       " 0.035033546,\n",
       " 0.17509022,\n",
       " 0.12693967,\n",
       " nan,\n",
       " 0.043052457,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.0883214,\n",
       " nan,\n",
       " nan,\n",
       " 0.335426,\n",
       " 0.09538187,\n",
       " 0.15007333,\n",
       " 0.2945106,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.17587827,\n",
       " nan,\n",
       " 0.3287742,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.22034915,\n",
       " nan,\n",
       " 0.19159219,\n",
       " nan,\n",
       " 0.18732339,\n",
       " nan,\n",
       " 0.05830287,\n",
       " 0.12606622,\n",
       " 0.14506157,\n",
       " 0.08970188,\n",
       " nan,\n",
       " nan,\n",
       " 0.3346337,\n",
       " 0.29457194,\n",
       " 0.15493655,\n",
       " nan,\n",
       " 0.120014,\n",
       " 0.23185015,\n",
       " 0.3093161,\n",
       " 0.38640064,\n",
       " nan,\n",
       " 0.083684176,\n",
       " nan,\n",
       " 0.04755249,\n",
       " 0.040877588,\n",
       " 0.2367089,\n",
       " 0.08066699,\n",
       " 0.13970956,\n",
       " 0.12989078,\n",
       " nan,\n",
       " 0.15933308,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.03342876,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 0.30009046,\n",
       " 0.1651634,\n",
       " nan,\n",
       " 0.2742775,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1.0,\n",
       " 0.15845788,\n",
       " nan,\n",
       " 0.21462761,\n",
       " 0.3332693,\n",
       " 0.55850005,\n",
       " 0.19095954,\n",
       " 0.21653618,\n",
       " -0.04067158,\n",
       " nan,\n",
       " 0.21253741,\n",
       " 0.23739354,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " -0.013130618,\n",
       " 0.3510212,\n",
       " nan,\n",
       " 0.20802557,\n",
       " nan]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f1591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preds'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aa5c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna() # remove the words that are not part of our word2vec's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637b682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.07769054899203413\n",
      "Pearson correlation: 0.15669632206080575\n"
     ]
    }
   ],
   "source": [
    "print(\"Spearman correlation:\", spearmanr(df['preds'], df['Human (Mean)'])[0])\n",
    "print(\"Pearson correlation:\", pearsonr(df['preds'], df['Human (Mean)'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a53992",
   "metadata": {},
   "source": [
    "We can see that there is low correlation between the similarity scores from the SPPMI-SVD vectors and the human scores, indicating poor performance of the model.\n",
    "\n",
    "This is expected as the 10k tweets we used probably did not contain enough data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natural-language-processing-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
