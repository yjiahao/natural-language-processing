---
title: "glove"
author: "Jia Hao"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Import libraries

```{r}
library(text2vec)
library(readr)
library(stringr)
library(ggplot2)
library(dplyr)
```

## Read and preprocess data

```{r}
df = read.csv(
  '../data/twitter/train_processed.csv',
  header=TRUE,
  encoding='latin-1'
)

contents = df$processed_text[1:50000]

# Remove square brackets and quotes
df$processed_text <- str_remove_all(df$processed_text, "\\[|\\]|'")

# Split by comma or space
df$processed_text <- str_split(df$processed_text, ",\\s*")

tokenized_sentences = df$processed_text
```

## Fitting the glove model

```{r}
# Create an iterator over the tokens
it <- itoken(tokenized_sentences, progressbar = TRUE)

# Build the vocabulary
vocab <- create_vocabulary(it)

# tune the model hyperparameters
i = 1
glove_vectors = list()
for (window in c(5, 10, 15)){
 for (rank in c(128, 256, 300)){
    # Create a term-co-occurrence matrix (TCM)
    tcm <- create_tcm(it, vectorizer = vocab_vectorizer(vocab), skip_grams_window = window)

    # Define the GloVe model
    glove <- GlobalVectors$new(rank = rank, x_max = 10)  # rank = embedding dimensions

    print(paste("Window length: ", window, ", Vector dimensions: ", rank))

    # Fit the GloVe model
    word_vectors <- glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01)

    # Combine word and context embeddings (optional)
    word_vectors <- word_vectors + t(glove$components)

    glove_vectors[[i]] = word_vectors
    i = i + 1
 }
}
```

## Analysis

### Evaluation of quality of embeddings of each model with Wordsim353

```{r}
# function to compute cosine similarity
cosine_sim <- function(w1, w2, embeddings) {
  words_in_glove <- rownames(embeddings)   # get vocab from current model
  if (!(w1 %in% words_in_glove) | !(w2 %in% words_in_glove)) {
    return(NA_real_)  # OOV handling
  }
  v1 <- embeddings[w1, ]
  v2 <- embeddings[w2, ]
  sum(v1 * v2) / (sqrt(sum(v1 * v1)) * sqrt(sum(v2 * v2)))
}

# Load wordsim
wordsim <- read.csv('../data/evaluation/wordsim353crowd.csv')
colnames(wordsim) <- c("word1", "word2", "score")

# Loop over models
results_all <- list()
i <- 1
for (wv in glove_vectors) {
  results <- wordsim %>%
    rowwise() %>%
    mutate(
      model_sim = cosine_sim(word1, word2, wv)
    ) %>%
    ungroup()
  
  # Filter out OOV pairs
  results_filtered <- results %>% filter(!is.na(model_sim))
  
  pearson_corr <- cor(results_filtered$score, results_filtered$model_sim, method = "pearson")
  spearman_corr <- cor(results_filtered$score, results_filtered$model_sim, method = "spearman")
  
  cat("Model", i, 
      "- Pearson:", round(pearson_corr, 3), 
      "Spearman:", round(spearman_corr, 3), "\n")
  
  results_all[[i]] <- list(pearson = pearson_corr, spearman = spearman_corr)
  i <- i + 1
}

```

Select model 2 as it has the highest Spearman correlation.
Model 2 has parameters: Window length = 5 , Vector dimensions = 256

### Nearest Neighbours for selected words

```{r}

selected_words = c(
    'sad',
    'chocolate',
    'chemistry',
    'month',
    'news'
)

best_wv = glove_vectors[[2]]

for (word in selected_words) {
  cat('Word is:', word, '\n')
  
  # Get vector for this word
  vec <- best_wv[word, , drop = FALSE]
  
  # Compute cosine similarity with all other words
  cos_sim = sim2(x = best_wv, y = vec, method = "cosine", norm = "l2")
  # filter for words not the same as this word
  cos_sim = cos_sim[rownames(cos_sim) != word, , drop = FALSE]
  
  print(head(sort(cos_sim[,1], decreasing = TRUE), 10))
  
  cat('\n')
    
}
```

Nearest words for 'sad' sometimes make sense.
Nearest words for 'chemistry', 'chocolate', 'month' and 'news' do not really make much sense.

## Visualization of PCA embedding space

```{r}
best_wv = glove_vectors[[2]]

# Perform PCA on the full word vectors
pca <- prcomp(best_wv, center = TRUE, scale. = TRUE)

# Create a data frame with the first 2 principal components
word_vectors_pca <- data.frame(pca$x[, 1:2])
word_vectors_pca$word <- rownames(best_wv)

# Randomly sample 200 words
set.seed(1)  # for reproducibility
word_vectors_pca_subset <- word_vectors_pca[sample(nrow(word_vectors_pca), 200), ]

# Plot the embeddings
ggplot(word_vectors_pca_subset, aes(x = PC1, y = PC2, label = word)) +
  geom_point(color='steelblue') +
  geom_text(aes(label = word), hjust = 0, vjust = 1, size = 3) +
  theme_minimal() +
  labs(title = "Word Embeddings Visualization (Random 200 Words after PCA)", 
       x = "PC1", y = "PC2")
```

No obvious clusters, we can see that the words are mostly scattered over the plot.

Some words close to each other do not really make sense, like 'songz' and 'soup', or 'schooool' and 'multiplayer'.
